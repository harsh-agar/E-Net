{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harsh-agar/E-Net/blob/master/Frustum_V2_func.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knUV6yx6MZ3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==1.13.1\n",
        "!pip install hyperopt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zitV5KxZMjId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL0FgJvzpfsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ##Test TF-GPU is working fine\n",
        "# # Creates a graph.\n",
        "# with tf.device('/gpu:0'):\n",
        "#   a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
        "#   b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
        "# c = tf.matmul(a, b)\n",
        "# # Creates a session with log_device_placement set to True.\n",
        "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "# # Runs the op.\n",
        "# print(sess.run(c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq6aOp5NNPVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/pointnet2/tf_ops/sampling')\n",
        "!bash tf_sampling_compile.sh\n",
        "os.chdir('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master//pointnet2/tf_ops/grouping')\n",
        "!bash tf_grouping_compile.sh\n",
        "os.chdir('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master//pointnet2/tf_ops/3d_interpolation')\n",
        "!bash tf_interpolate_compile.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOtbERWZQwUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "BASE_DIR = os.path.dirname('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/pointnet2/')\n",
        "ROOT_DIR = os.path.dirname('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/pointnet2/utils/')\n",
        "sys.path.append(BASE_DIR)\n",
        "sys.path.append(ROOT_DIR)\n",
        "sys.path.append('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/pointnet2/tf_ops/sampling/')\n",
        "sys.path.append('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/pointnet2/tf_ops/grouping/')\n",
        "sys.path.append('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/pointnet2/tf_ops/3d_interpolation/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRq2mBn1jCZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "import tf_util\n",
        "import numpy as np\n",
        "import importlib\n",
        "import csv\n",
        "from pointnet_util import pointnet_sa_module, pointnet_sa_module_msg, pointnet_fp_module"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm1vPBRrGegV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_HEADING_BIN = 12\n",
        "NUM_SIZE_CLUSTER = 8 # one cluster for each type\n",
        "VAl_CONSTANT = 3 # 3 if 'x,y,z', 2 if 'x,y'\n",
        "VAL_NUM_HEADING_BIN = 2\n",
        "VAL_NUM_SIZE_CLUSTER = 4 # 4 if 'x,y,z', 3 if 'x,y'\n",
        "\n",
        "# Set training configurations\n",
        "BATCH_SIZE = 1\n",
        "NUM_POINT = 1024\n",
        "NUM_CLASSES = 2\n",
        "NUM_CHANNEL = 4\n",
        "NUM_OBJECT_POINT = 512\n",
        "xyz_only = 1\n",
        "\n",
        "batch_size = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsKwdP5zgIIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_gather_object_pc(point_cloud, mask, npoints=512):\n",
        "\t''' Gather object point clouds according to predicted masks.\n",
        "\tInput:\n",
        "\t\tpoint_cloud: TF tensor in shape (B,N,C)\n",
        "\t\tmask: TF tensor in shape (B,N) of 0 (not pick) or 1 (pick)\n",
        "\t\tnpoints: int scalar, maximum number of points to keep (default: 512)\n",
        "\tOutput:\n",
        "\t\tobject_pc: TF tensor in shape (B,npoint,C)\n",
        "\t\tindices: TF int tensor in shape (B,npoint,2)\n",
        "\t'''\n",
        "\tdef mask_to_indices(mask):\n",
        "\t\tindices = np.zeros((mask.shape[0], npoints, 2), dtype=np.int32)\n",
        "\t\tfor i in range(mask.shape[0]):\n",
        "\t\t\tpos_indices = np.where(mask[i,:]>0.5)[0] #BxN\n",
        "\t\t\t# skip cases when pos_indices is empty\n",
        "\t\t\tif len(pos_indices) > 0: \n",
        "\t\t\t\tif len(pos_indices) > npoints:\n",
        "\t\t\t\t\tchoice = np.random.choice(len(pos_indices),\n",
        "\t\t\t\t\t\tnpoints, replace=False)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tchoice = np.random.choice(len(pos_indices),\n",
        "\t\t\t\t\t\tnpoints-len(pos_indices), replace=True)\n",
        "\t\t\t\t\tchoice = np.concatenate((np.arange(len(pos_indices)), choice))\n",
        "\t\t\t\tnp.random.shuffle(choice)\n",
        "\t\t\t\tindices[i,:,1] = pos_indices[choice]\n",
        "\t\t\tindices[i,:,0] = i\n",
        "\t\treturn indices\n",
        "\n",
        "\tindices = tf.py_func(mask_to_indices, [mask], tf.int32)  \n",
        "\tobject_pc = tf.gather_nd(point_cloud, indices)\n",
        "\treturn object_pc, indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3ZCznS5YL-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_instance_seg_v2_net(point_cloud, is_training, bn_decay, end_points):\n",
        "  l0_xyz = tf.slice(point_cloud, [0,0,0], [-1,-1,3])\n",
        "  l0_points = tf.slice(point_cloud, [0,0,3], [-1,-1,1])\n",
        "\n",
        "  # Set abstraction layers\n",
        "  l1_xyz, l1_points = pointnet_sa_module_msg(l0_xyz, l0_points,\n",
        "      128, [0.2,0.4,0.8], [32,64,128],\n",
        "      [[32,32,64], [64,64,128], [64,96,128]],\n",
        "      is_training, bn_decay, scope='layer1')\n",
        "  l2_xyz, l2_points = pointnet_sa_module_msg(l1_xyz, l1_points,\n",
        "      32, [0.4,0.8,1.6], [64,64,128],\n",
        "      [[64,64,128], [128,128,256], [128,128,256]],\n",
        "      is_training, bn_decay, scope='layer2')\n",
        "  l3_xyz, l3_points, _ = pointnet_sa_module(l2_xyz, l2_points,\n",
        "      npoint=None, radius=None, nsample=None, mlp=[128,256,1024],\n",
        "      mlp2=None, group_all=True, is_training=is_training,\n",
        "      bn_decay=bn_decay, scope='layer3')\n",
        "  net = tf_util.fully_connected(tf.reshape(l3_points, [batch_size, 1024]), 512, scope='fc1-classification', bn=True, is_training=is_training, bn_decay=bn_decay)\n",
        "  net = tf_util.fully_connected(net, 256, scope='fc2-classification', bn=True,\n",
        "      is_training=is_training, bn_decay=bn_decay)\n",
        "  predicted_classes_scores = tf_util.fully_connected(net, NUM_CLASSES, activation_fn=tf.nn.softmax,\n",
        "      scope='fc3-classification')\n",
        "\n",
        "  l3_points = tf.concat([l3_points, tf.expand_dims(predicted_classes_scores, 1)], axis=2)\n",
        "\n",
        "  # Segmentation\n",
        "  l2_points = pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points,\n",
        "      [128,128], is_training, bn_decay, scope='fa_layer1')\n",
        "  l1_points = pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points,\n",
        "      [128,128], is_training, bn_decay, scope='fa_layer2')\n",
        "  l0_points = pointnet_fp_module(l0_xyz, l1_xyz,\n",
        "      tf.concat([l0_xyz,l0_points],axis=-1), l1_points,\n",
        "      [128,128], is_training, bn_decay, scope='fa_layer3')\n",
        "\n",
        "  # FC layers\n",
        "  net = tf_util.conv1d(l0_points, 128, 1, padding='VALID', bn=True,\n",
        "      is_training=is_training, scope='conv1d-fc1', bn_decay=bn_decay)\n",
        "  end_points['feats'] = net \n",
        "  net = tf_util.dropout(net, keep_prob=0.7,\n",
        "      is_training=is_training, scope='dp1')\n",
        "  logits = tf_util.conv1d(net, 2, 1,\n",
        "      padding='VALID', activation_fn=None, scope='conv1d-fc2')\n",
        "  return predicted_classes_scores, logits, end_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTqTeG04YMJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_cloud_masking(point_cloud, logits, end_points, xyz_only=True):\n",
        "    num_point = point_cloud.get_shape()[1].value\n",
        "    batch_size = point_cloud.get_shape()[0].value\n",
        "    mask = tf.slice(logits,[0,0,0],[-1,-1,1]) < \\\n",
        "        tf.slice(logits,[0,0,1],[-1,-1,1]) #True when object is present else false\n",
        "    mask = tf.cast(mask, tf.float32) # BxNx1\n",
        "    mask_count = tf.tile(tf.reduce_sum(mask,axis=1,keep_dims=True),\n",
        "        [1,1,3]) # Bx1x3\n",
        "    point_cloud_xyz = tf.slice(point_cloud, [0,0,0], [-1,-1,3]) # BxNx3\n",
        "    mask_xyz_mean = tf.reduce_sum(tf.tile(mask, [1,1,3])*point_cloud_xyz,\n",
        "        axis=1, keep_dims=True) # Bx1x3 ##Computes mean of all the mask points 'x,y,z' which have a object 'mask' present in it\n",
        "    mask = tf.squeeze(mask, axis=[2]) # BxN\n",
        "    mask_xyz_mean = mask_xyz_mean/tf.maximum(mask_count,1) # Bx1x3\n",
        "\n",
        "    # Translate to masked points' centroid\n",
        "    point_cloud_xyz_stage1 = point_cloud_xyz - \\\n",
        "        tf.tile(mask_xyz_mean, [1,num_point,1])\n",
        "    xyz_only = 1\n",
        "    if xyz_only: \n",
        "        point_cloud_stage1 = point_cloud_xyz_stage1\n",
        "    else:\n",
        "        point_cloud_features = tf.slice(point_cloud, [0,0,3], [-1,-1,-1])\n",
        "        point_cloud_stage1 = tf.concat(\\\n",
        "            [point_cloud_xyz_stage1, point_cloud_features], axis=-1)\n",
        "    num_channels = point_cloud_stage1.get_shape()[2].value\n",
        "    object_point_cloud, _ = tf_gather_object_pc(point_cloud_stage1,\n",
        "        mask, NUM_OBJECT_POINT)\n",
        "    object_point_cloud.set_shape([batch_size, NUM_OBJECT_POINT, num_channels])\n",
        "    return object_point_cloud, tf.squeeze(mask_xyz_mean, axis=1), end_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o0-B1PjYMSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_center_regression_net(object_point_cloud, predicted_classes_scores,\n",
        "                              is_training, bn_decay, end_points):\n",
        "    num_point = object_point_cloud.get_shape()[1].value\n",
        "    net = tf.expand_dims(object_point_cloud, 2)\n",
        "    net = tf_util.conv2d(net, 128, [1,1],\n",
        "                         padding='VALID', stride=[1,1],\n",
        "                         bn=True, is_training=is_training,\n",
        "                         scope='conv-reg1-stage1', bn_decay=bn_decay)\n",
        "    net = tf_util.conv2d(net,  128, [1,1],\n",
        "                         padding='VALID', stride=[1,1],\n",
        "                         bn=True, is_training=is_training,\n",
        "                         scope='conv-reg2-stage1', bn_decay=bn_decay)\n",
        "    net = tf_util.conv2d(net, 256, [1,1],\n",
        "                         padding='VALID', stride=[1,1],\n",
        "                         bn=True, is_training=is_training,\n",
        "                         scope='conv-reg3-stage1', bn_decay=bn_decay)\n",
        "    net = tf_util.max_pool2d(net, [num_point,1],\n",
        "        padding='VALID', scope='maxpool-stage1')\n",
        "    net = tf.squeeze(net, axis=[1,2])\n",
        "    net = tf.concat([net, predicted_classes_scores], axis=1)\n",
        "    net = tf_util.fully_connected(net, 256, scope='fc1-stage1', bn=True,\n",
        "        is_training=is_training, bn_decay=bn_decay)\n",
        "    net = tf_util.fully_connected(net, 128, scope='fc2-stage1', bn=True,\n",
        "        is_training=is_training, bn_decay=bn_decay)\n",
        "    predicted_center = tf_util.fully_connected(net, 3, activation_fn=None,\n",
        "        scope='fc3-stage1')\n",
        "    return predicted_center, end_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDJO8BytYMdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_3d_box_estimation_v2_net(object_point_cloud, predicted_classes_scores, is_training, bn_decay, end_points):\n",
        "  batch_size = object_point_cloud.get_shape()[0].value\n",
        "\n",
        "  l0_xyz = object_point_cloud\n",
        "  l0_points = None\n",
        "  # Set abstraction layers\n",
        "  l1_xyz, l1_points, l1_indices = pointnet_sa_module(l0_xyz, l0_points,\n",
        "      npoint=128, radius=0.2, nsample=64, mlp=[64,64,128],\n",
        "      mlp2=None, group_all=False,\n",
        "      is_training=is_training, bn_decay=bn_decay, scope='ssg-layer1')\n",
        "  l2_xyz, l2_points, l2_indices = pointnet_sa_module(l1_xyz, l1_points,\n",
        "      npoint=32, radius=0.4, nsample=64, mlp=[128,128,256],\n",
        "      mlp2=None, group_all=False,\n",
        "      is_training=is_training, bn_decay=bn_decay, scope='ssg-layer2')\n",
        "  l3_xyz, l3_points, l3_indices = pointnet_sa_module(l2_xyz, l2_points,\n",
        "      npoint=None, radius=None, nsample=None, mlp=[256,256,512],\n",
        "      mlp2=None, group_all=True,\n",
        "      is_training=is_training, bn_decay=bn_decay, scope='ssg-layer3')\n",
        "\n",
        "  # Fully connected layers\n",
        "  net = tf.reshape(l3_points, [batch_size, -1])\n",
        "  net = tf.concat([net, predicted_classes_scores], axis=1)\n",
        "  net = tf_util.fully_connected(net, 512, bn=True,\n",
        "      is_training=is_training, scope='fc1', bn_decay=bn_decay)\n",
        "  net = tf_util.fully_connected(net, 256, bn=True,\n",
        "      is_training=is_training, scope='fc2', bn_decay=bn_decay)\n",
        "\n",
        "  # The first 3 numbers: box center coordinates (cx,cy,cz),\n",
        "  # the next NUM_HEADING_BIN*2:  heading bin class scores and bin residuals\n",
        "  # next NUM_SIZE_CLUSTER*4: box cluster scores and residuals\n",
        "  output = tf_util.fully_connected(net,\n",
        "      VAl_CONSTANT+VAL_NUM_HEADING_BIN*NUM_HEADING_BIN+VAL_NUM_SIZE_CLUSTER*NUM_SIZE_CLUSTER, activation_fn=None, scope='fc3')\n",
        "  return output, end_points\n",
        "#   return l3_points, end_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDEzwEKmnLr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_output_to_tensors(output, end_points):\n",
        "    ''' Parse batch output to separate tensors (added to end_points)\n",
        "    Input:\n",
        "        output: TF tensor in shape (B,VAl_CONSTANT+VAL_NUM_HEADING_BIN*NUM_HEADING_BIN+VAL_NUM_SIZE_CLUSTER*NUM_SIZE_CLUSTER)\n",
        "        end_points: dict\n",
        "    Output:\n",
        "        end_points: dict (updated)\n",
        "    '''\n",
        "    batch_size = output.get_shape()[0].value\n",
        "    center = tf.slice(output, [0,0], [-1,VAl_CONSTANT])\n",
        "    end_points['center_boxnet'] = center\n",
        "\n",
        "    heading_scores = tf.slice(output, [0,VAl_CONSTANT], [-1,NUM_HEADING_BIN])\n",
        "    heading_residuals_normalized = tf.slice(output, [0,VAl_CONSTANT+NUM_HEADING_BIN],\n",
        "        [-1,NUM_HEADING_BIN])\n",
        "    end_points['heading_scores'] = heading_scores # BxNUM_HEADING_BIN\n",
        "    end_points['heading_residuals_normalized'] = \\\n",
        "        heading_residuals_normalized # BxNUM_HEADING_BIN (-1 to 1)\n",
        "    end_points['heading_residuals'] = \\\n",
        "        heading_residuals_normalized * (np.pi/NUM_HEADING_BIN) # BxNUM_HEADING_BIN\n",
        "    \n",
        "    size_scores = tf.slice(output, [0,VAl_CONSTANT+NUM_HEADING_BIN*VAL_NUM_HEADING_BIN],\n",
        "        [-1,NUM_SIZE_CLUSTER]) # BxNUM_SIZE_CLUSTER\n",
        "    size_residuals_normalized = tf.slice(output,\n",
        "        [0,VAl_CONSTANT+NUM_HEADING_BIN*VAL_NUM_HEADING_BIN+NUM_SIZE_CLUSTER], [-1,NUM_SIZE_CLUSTER*(VAL_NUM_SIZE_CLUSTER-1)])\n",
        "    size_residuals_normalized = tf.reshape(size_residuals_normalized,\n",
        "        [batch_size, NUM_SIZE_CLUSTER, (VAL_NUM_SIZE_CLUSTER-1)]) # BxNUM_SIZE_CLUSTERx3\n",
        "    end_points['size_scores'] = size_scores\n",
        "    end_points['size_residuals_normalized'] = size_residuals_normalized\n",
        "    '''\n",
        "    end_points['size_residuals'] = size_residuals_normalized * \\\n",
        "        tf.expand_dims(tf.constant(g_mean_size_arr, dtype=tf.float32), 0)\n",
        "    '''\n",
        "    end_points['size_residuals'] = size_residuals_normalized\n",
        "    return end_points"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSdVAXHTag66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(point_cloud, is_training, bn_decay=None):\n",
        "    end_points = {}\n",
        "    # 3D Instance Segmentation PointNet\n",
        "    predicted_classes_scores, logits, end_points = get_instance_seg_v2_net(point_cloud, is_training, bn_decay, end_points)\n",
        "    end_points['mask_logits'] = logits\n",
        "    end_points['class_logits'] = predicted_classes_scores\n",
        "    # Masking\n",
        "    # select masked points and translate to masked points' centroid\n",
        "    object_point_cloud_xyz, mask_xyz_mean, end_points = \\\n",
        "        point_cloud_masking(point_cloud, logits, end_points)\n",
        "\n",
        "    # T-Net and coordinate translation\n",
        "    center_delta, end_points = get_center_regression_net(\\\n",
        "        object_point_cloud_xyz, predicted_classes_scores,\n",
        "        is_training, bn_decay, end_points)\n",
        "    stage1_center = center_delta + mask_xyz_mean # Bx3\n",
        "    end_points['stage1_center'] = stage1_center\n",
        "    # Get object point cloud in object coordinate\n",
        "    object_point_cloud_xyz_new = \\\n",
        "        object_point_cloud_xyz - tf.expand_dims(center_delta, 1)\n",
        "\n",
        "#     Amodel Box Estimation PointNet\n",
        "    output, end_points = get_3d_box_estimation_v2_net(\\\n",
        "        object_point_cloud_xyz_new, predicted_classes_scores,\n",
        "        is_training, bn_decay, end_points)\n",
        "\n",
        "    # Parse output to 3D box parameters\n",
        "    end_points = parse_output_to_tensors(output, end_points)\n",
        "    end_points['center'] = end_points['center_boxnet'] + stage1_center # Bx3\n",
        "\n",
        "    return end_points\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4b6x4nZaNXC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c1a5760-7300-4d11-b275-8e833e79b643"
      },
      "source": [
        "BATCH_SIZE = 1\n",
        "NUM_POINT = 1024\n",
        "# MODEL = importlib.import_module('frustum_pointnets_v1')\n",
        "NUM_CLASSES = 2\n",
        "NUM_CHANNEL = 4\n",
        "batch_size = 1\n",
        "\n",
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default():\n",
        "  is_training_pl = tf.placeholder(tf.bool, shape=())\n",
        "  pointclouds_pl = tf.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_POINT, 6))\n",
        "  batch_size = pointclouds_pl.get_shape()[0].value\n",
        "  num_point = pointclouds_pl.get_shape()[1].value        \n",
        "  is_training = tf.constant(False, dtype=tf.bool)\n",
        "\n",
        "  end_points = get_model(pointclouds_pl, is_training_pl)\n",
        "\n",
        "  sess = tf.Session()\n",
        "  # with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  pc = np.loadtxt(open(\"/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/data_sample.csv\", \"rb\"), delimiter=\",\")\n",
        "  ops = {'pointclouds_pl': pointclouds_pl,\n",
        "           'is_training_pl': is_training_pl}\n",
        "  feed_dict = {ops['pointclouds_pl']: [pc], ops['is_training_pl']: False}\n",
        "\n",
        "  end_points = sess.run(end_points, feed_dict=feed_dict)\n",
        "  print(end_points)\n",
        "\n",
        "  for key, value in end_points.items() :\n",
        "      print (key)\n",
        "      print (value.shape)\n",
        "      print (\"\\n\")"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'feats': array([[[0.        , 3.2328126 , 3.2665458 , ..., 0.        ,\n",
            "         0.        , 0.        ],\n",
            "        [0.23080653, 1.5711966 , 2.96782   , ..., 0.90524364,\n",
            "         0.6365163 , 0.        ],\n",
            "        [0.        , 4.953106  , 2.333268  , ..., 0.        ,\n",
            "         0.        , 0.        ],\n",
            "        ...,\n",
            "        [0.6596206 , 1.72573   , 1.6862521 , ..., 0.6372791 ,\n",
            "         0.2695727 , 0.        ],\n",
            "        [0.        , 2.4600356 , 0.        , ..., 0.        ,\n",
            "         0.        , 0.        ],\n",
            "        [0.        , 2.7059364 , 0.        , ..., 0.        ,\n",
            "         0.        , 0.        ]]], dtype=float32), 'mask_logits': array([[[ 0.7834685 , -2.5934873 ],\n",
            "        [ 1.0725826 , -1.4713265 ],\n",
            "        [ 1.1804701 , -4.087547  ],\n",
            "        ...,\n",
            "        [ 0.62612945, -1.2759098 ],\n",
            "        [ 0.55096453, -5.277473  ],\n",
            "        [ 0.88969475, -3.034563  ]]], dtype=float32), 'class_logits': array([[0.8410748 , 0.15892518]], dtype=float32), 'stage1_center': array([[-1.0210165, -1.8655769, -0.5400567]], dtype=float32), 'center_boxnet': array([[-1.4163834,  0.4251079, -1.1744123]], dtype=float32), 'heading_scores': array([[-2.6392093 , -1.6104511 , -2.2373922 ,  0.6598702 ,  0.5089073 ,\n",
            "         0.41538137,  4.6972265 , -5.9844007 , -2.1485367 ,  2.3949397 ,\n",
            "         0.59470665, -0.82358503]], dtype=float32), 'heading_residuals_normalized': array([[ 1.8204802 ,  1.5332799 ,  0.6741674 , -0.27770126, -0.08056206,\n",
            "         0.6554741 , -1.9145784 ,  0.14767826,  2.0909703 , -0.6896435 ,\n",
            "        -0.6422672 ,  1.0991082 ]], dtype=float32), 'heading_residuals': array([[ 0.47660062,  0.40141174,  0.17649661, -0.07270202, -0.0210911 ,\n",
            "         0.17160273, -0.5012355 ,  0.03866208,  0.5474148 , -0.18054825,\n",
            "        -0.16814516,  0.28774586]], dtype=float32), 'size_scores': array([[ 0.05735272, -1.2223644 ,  1.4369688 , -1.9101877 , -0.5627834 ,\n",
            "         0.15157783,  3.398141  , -0.6661902 ]], dtype=float32), 'size_residuals_normalized': array([[[-1.0611848 ,  0.95546144, -0.7407543 ],\n",
            "        [ 0.87998915,  0.26329553,  0.3704365 ],\n",
            "        [-1.5946906 ,  2.8012464 , -0.09573406],\n",
            "        [-3.5313196 , -0.42041576,  0.2238257 ],\n",
            "        [ 0.7240975 ,  1.6896219 , -0.18055409],\n",
            "        [-0.14903398, -0.07365857,  0.9957616 ],\n",
            "        [ 1.527853  , -0.26351774, -0.08863401],\n",
            "        [-0.5920344 , -0.54837334,  1.0784305 ]]], dtype=float32), 'size_residuals': array([[[-1.0611848 ,  0.95546144, -0.7407543 ],\n",
            "        [ 0.87998915,  0.26329553,  0.3704365 ],\n",
            "        [-1.5946906 ,  2.8012464 , -0.09573406],\n",
            "        [-3.5313196 , -0.42041576,  0.2238257 ],\n",
            "        [ 0.7240975 ,  1.6896219 , -0.18055409],\n",
            "        [-0.14903398, -0.07365857,  0.9957616 ],\n",
            "        [ 1.527853  , -0.26351774, -0.08863401],\n",
            "        [-0.5920344 , -0.54837334,  1.0784305 ]]], dtype=float32), 'center': array([[-2.4373999, -1.440469 , -1.714469 ]], dtype=float32)}\n",
            "feats\n",
            "(1, 1024, 128)\n",
            "\n",
            "\n",
            "mask_logits\n",
            "(1, 1024, 2)\n",
            "\n",
            "\n",
            "class_logits\n",
            "(1, 2)\n",
            "\n",
            "\n",
            "stage1_center\n",
            "(1, 3)\n",
            "\n",
            "\n",
            "center_boxnet\n",
            "(1, 3)\n",
            "\n",
            "\n",
            "heading_scores\n",
            "(1, 12)\n",
            "\n",
            "\n",
            "heading_residuals_normalized\n",
            "(1, 12)\n",
            "\n",
            "\n",
            "heading_residuals\n",
            "(1, 12)\n",
            "\n",
            "\n",
            "size_scores\n",
            "(1, 8)\n",
            "\n",
            "\n",
            "size_residuals_normalized\n",
            "(1, 8, 3)\n",
            "\n",
            "\n",
            "size_residuals\n",
            "(1, 8, 3)\n",
            "\n",
            "\n",
            "center\n",
            "(1, 3)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cckA0hxcHIDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.reset_default_graph()\n",
        "# with tf.Graph().as_default():\n",
        "# #   with tf.device('/gpu:0'):\n",
        "#   is_training = tf.constant(False, dtype=tf.bool)\n",
        "#   bn_decay=None\n",
        "#   point_cloud = tf.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_POINT, 6))\n",
        "#   batch_size = point_cloud.get_shape()[0].value\n",
        "#   num_point = point_cloud.get_shape()[1].value\n",
        "#   # Classification\n",
        "#   l0_xyz = tf.slice(point_cloud, [0,0,0], [-1,-1,3])\n",
        "#   l0_points = tf.slice(point_cloud, [0,0,3], [-1,-1,1])\n",
        "\n",
        "#   # Set abstraction layers\n",
        "#   l1_xyz, l1_points = pointnet_sa_module_msg(l0_xyz, l0_points,\n",
        "#       128, [0.2,0.4,0.8], [32,64,128],\n",
        "#       [[32,32,64], [64,64,128], [64,96,128]],\n",
        "#       is_training, bn_decay, scope='layer1')\n",
        "#   l2_xyz, l2_points = pointnet_sa_module_msg(l1_xyz, l1_points,\n",
        "#       32, [0.4,0.8,1.6], [64,64,128],\n",
        "#       [[64,64,128], [128,128,256], [128,128,256]],\n",
        "#       is_training, bn_decay, scope='layer2')\n",
        "#   l3_xyz, l3_points, _ = pointnet_sa_module(l2_xyz, l2_points,\n",
        "#       npoint=None, radius=None, nsample=None, mlp=[128,256,1024],\n",
        "#       mlp2=None, group_all=True, is_training=is_training,\n",
        "#       bn_decay=bn_decay, scope='layer3')\n",
        "#   net = tf_util.fully_connected(tf.reshape(l3_points, [batch_size, 1024]), 512, scope='fc1-classification', bn=True, is_training=is_training, bn_decay=bn_decay)\n",
        "#   net = tf_util.fully_connected(net, 256, scope='fc2-classification', bn=True,\n",
        "#       is_training=is_training, bn_decay=bn_decay)\n",
        "#   predicted_classes_scores = tf_util.fully_connected(net, NUM_CLASSES, activation_fn=tf.nn.softmax,\n",
        "#       scope='fc3-classification')\n",
        "\n",
        "#   l3_points = tf.concat([l3_points, tf.expand_dims(predicted_classes_scores, 1)], axis=2)\n",
        "\n",
        "#   # Segmentation\n",
        "#   l2_points = pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points,\n",
        "#       [128,128], is_training, bn_decay, scope='fa_layer1')\n",
        "#   l1_points = pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points,\n",
        "#       [128,128], is_training, bn_decay, scope='fa_layer2')\n",
        "#   l0_points = pointnet_fp_module(l0_xyz, l1_xyz,\n",
        "#       tf.concat([l0_xyz,l0_points],axis=-1), l1_points,\n",
        "#       [128,128], is_training, bn_decay, scope='fa_layer3')\n",
        "\n",
        "#   # FC layers\n",
        "#   net = tf_util.conv1d(l0_points, 128, 1, padding='VALID', bn=True,\n",
        "#       is_training=is_training, scope='conv1d-fc1', bn_decay=bn_decay)\n",
        "#   net = tf_util.dropout(net, keep_prob=0.7,\n",
        "#       is_training=is_training, scope='dp1')\n",
        "#   logits = tf_util.conv1d(net, 2, 1,\n",
        "#       padding='VALID', activation_fn=None, scope='conv1d-fc2')\n",
        "\n",
        "#   # Masking\n",
        "#   mask = tf.slice(logits,[0,0,0],[-1,-1,1]) < \\\n",
        "#     tf.slice(logits,[0,0,1],[-1,-1,1]) #True when object is present else false\n",
        "#   mask = tf.cast(mask, tf.float32) # BxNx1\n",
        "#   mask_count = tf.tile(tf.reduce_sum(mask,axis=1,keep_dims=True),\n",
        "#     [1,1,3]) # Bx1x3\n",
        "#   point_cloud_xyz = tf.slice(point_cloud, [0,0,0], [-1,-1,3]) # BxNx3\n",
        "#   mask_xyz_mean = tf.reduce_sum(tf.tile(mask, [1,1,3])*point_cloud_xyz,\n",
        "#     axis=1, keep_dims=True) # Bx1x3 ##Computes mean of all the mask points 'x,y,z' which have a object 'mask' present in it\n",
        "#   mask = tf.squeeze(mask, axis=[2]) # BxN\n",
        "#   mask_xyz_mean = mask_xyz_mean/tf.maximum(mask_count,1) # Bx1x3\n",
        "\n",
        "#   # Translate to masked points' centroid\n",
        "#   point_cloud_xyz_stage1 = point_cloud_xyz - \\\n",
        "#     tf.tile(mask_xyz_mean, [1,num_point,1])\n",
        "#   if xyz_only: \n",
        "#     point_cloud_stage1 = point_cloud_xyz_stage1\n",
        "#   else:\n",
        "#     point_cloud_features = tf.slice(point_cloud, [0,0,3], [-1,-1,-1])\n",
        "#     point_cloud_stage1 = tf.concat(\\\n",
        "#       [point_cloud_xyz_stage1, point_cloud_features], axis=-1)\n",
        "#   num_channels = point_cloud_stage1.get_shape()[2].value\n",
        "#   object_point_cloud, _ = tf_gather_object_pc(point_cloud_stage1,\n",
        "#     mask, NUM_OBJECT_POINT)\n",
        "#   object_point_cloud.set_shape([batch_size, NUM_OBJECT_POINT, num_channels])\n",
        "#   #T-Net\n",
        "#   num_point = object_point_cloud.get_shape()[1].value\n",
        "#   net = tf.expand_dims(object_point_cloud, 2)\n",
        "#   net = tf_util.conv2d(net, 128, [1,1],\n",
        "#              padding='VALID', stride=[1,1],\n",
        "#              bn=True, is_training=is_training,\n",
        "#              scope='conv-reg1-stage1', bn_decay=bn_decay)\n",
        "#   net = tf_util.conv2d(net,  128, [1,1],\n",
        "#              padding='VALID', stride=[1,1],\n",
        "#              bn=True, is_training=is_training,\n",
        "#              scope='conv-reg2-stage1', bn_decay=bn_decay)\n",
        "#   net = tf_util.conv2d(net, 256, [1,1],\n",
        "#              padding='VALID', stride=[1,1],\n",
        "#              bn=True, is_training=is_training,\n",
        "#              scope='conv-reg3-stage1', bn_decay=bn_decay)\n",
        "#   net = tf_util.max_pool2d(net, [num_point,1],\n",
        "#     padding='VALID', scope='maxpool-stage1')\n",
        "#   net = tf.squeeze(net, axis=[1,2])\n",
        "#   net = tf.concat([net, predicted_classes_scores], axis=1)\n",
        "#   net = tf_util.fully_connected(net, 256, scope='fc1-stage1', bn=True,\n",
        "#     is_training=is_training, bn_decay=bn_decay)\n",
        "#   net = tf_util.fully_connected(net, 128, scope='fc2-stage1', bn=True,\n",
        "#     is_training=is_training, bn_decay=bn_decay)\n",
        "#   predicted_center = tf_util.fully_connected(net, 3, activation_fn=None,\n",
        "#     scope='fc3-stage1')\n",
        "#   # Translation\n",
        "#   stage1_center = predicted_center + mask_xyz_mean # Bx3\n",
        "\n",
        "#   # Get object point cloud in object coordinate\n",
        "#   object_point_cloud_xyz_new = \\\n",
        "#     object_point_cloud - tf.expand_dims(predicted_center, 1)  \n",
        "  \n",
        "#   # Amodal\n",
        "#   batch_size = object_point_cloud_xyz_new.get_shape()[0].value\n",
        "\n",
        "#   l0_xyz = object_point_cloud_xyz_new\n",
        "#   l0_points = None\n",
        "#   # Set abstraction layers\n",
        "#   l1_xyz, l1_points, l1_indices = pointnet_sa_module(l0_xyz, l0_points,\n",
        "#       npoint=128, radius=0.2, nsample=64, mlp=[64,64,128],\n",
        "#       mlp2=None, group_all=False,\n",
        "#       is_training=is_training, bn_decay=bn_decay, scope='ssg-layer1')\n",
        "#   l2_xyz, l2_points, l2_indices = pointnet_sa_module(l1_xyz, l1_points,\n",
        "#       npoint=32, radius=0.4, nsample=64, mlp=[128,128,256],\n",
        "#       mlp2=None, group_all=False,\n",
        "#       is_training=is_training, bn_decay=bn_decay, scope='ssg-layer2')\n",
        "#   l3_xyz, l3_points, l3_indices = pointnet_sa_module(l2_xyz, l2_points,\n",
        "#       npoint=None, radius=None, nsample=None, mlp=[256,256,512],\n",
        "#       mlp2=None, group_all=True,\n",
        "#       is_training=is_training, bn_decay=bn_decay, scope='ssg-layer3')\n",
        "\n",
        "#   # Fully connected layers\n",
        "#   net = tf.reshape(l3_points, [batch_size, -1])\n",
        "#   net = tf.concat([net, predicted_classes_scores], axis=1)\n",
        "#   net = tf_util.fully_connected(net, 512, bn=True,\n",
        "#       is_training=is_training, scope='fc1', bn_decay=bn_decay)\n",
        "#   net = tf_util.fully_connected(net, 256, bn=True,\n",
        "#       is_training=is_training, scope='fc2', bn_decay=bn_decay)\n",
        "\n",
        "#   # The first 3 numbers: box center coordinates (cx,cy,cz),\n",
        "#   # the next NUM_HEADING_BIN*2:  heading bin class scores and bin residuals\n",
        "#   # next NUM_SIZE_CLUSTER*4: box cluster scores and residuals\n",
        "#   output = tf_util.fully_connected(net,\n",
        "#       VAl_CONSTANT+VAL_NUM_HEADING_BIN*NUM_HEADING_BIN+VAL_NUM_SIZE_CLUSTER*NUM_SIZE_CLUSTER, activation_fn=None, scope='fc3')  \n",
        "#   #################################\n",
        "\n",
        "#   sess = tf.Session()\n",
        "#   # with tf.Session() as sess:\n",
        "#   sess.run(tf.global_variables_initializer())\n",
        "#   pc = np.loadtxt(open(\"/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/data_sample.csv\", \"rb\"), delimiter=\",\")\n",
        "# #   print (pc)\n",
        "#   ops = {'pointclouds_pl': point_cloud,\n",
        "#            'is_training_pl': is_training}\n",
        "# #     feed_dict = {ops['pointclouds_pl']: [pc], ops['is_training_pl']: False}\n",
        "#   feed_dict = {ops['pointclouds_pl']: [pc]}\n",
        "#   batch_size_scores = sess.run(output, feed_dict=feed_dict)\n",
        "#   print(batch_size_scores.shape)\n",
        "#   print(batch_size_scores)\n",
        "#   # \t\tprint(VAl_CONSTANT+VAL_NUM_HEADING_BIN*NUM_HEADING_BIN+VAL_NUM_SIZE_CLUSTER*NUM_SIZE_CLUSTER)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KRmQJz-JYPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Generate random .csv file\n",
        "\n",
        "# import random\n",
        "# import csv\n",
        "\n",
        "# with open('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/data_sample.csv', 'w', newline='') as writeFile:\n",
        "# \twriter = csv.writer(writeFile)\n",
        "# \tfor x in range(1024):\n",
        "# \t\tline = \t[0,0,0,0.0,0.0,0.0]\n",
        "# \t\tline[0] = (random.randint(0,100))\n",
        "# \t\tline[1] = (random.randint(0,100))\n",
        "# \t\tline[2] = (random.randint(0,100))\n",
        "# \t\tline[3] = (random.random())\n",
        "# \t\tline[4] = (random.random())\n",
        "# \t\tline[5] = (random.random())\n",
        "# # \t\tprint (line)\n",
        "# \t\twriter.writerow(line)\n",
        "# pc = np.loadtxt(open(\"/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/data_sample.csv\", \"rb\"), delimiter=\",\")\n",
        "# print (pc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}