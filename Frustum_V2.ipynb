{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harsh-agar/E-Net/blob/master/Frustum_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knUV6yx6MZ3D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "6d78692d-5d65-4d6f-f94d-1edec1f412c7"
      },
      "source": [
        "!pip install tensorflow-gpu==1.13.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==1.13.1 in /usr/local/lib/python3.6/dist-packages (1.13.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.33.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.16.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.13.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.1) (3.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1oK0b8ZSbVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "57cd809d-5ba4-4c2f-8518-df95fbd5dcda"
      },
      "source": [
        "!pip install hyperopt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (0.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.3.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt) (2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.16.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from hyperopt) (1.12.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt) (3.8.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt) (4.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zitV5KxZMjId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "243d1b09-ab69-4705-8737-aedda3bd8b9f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq6aOp5NNPVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "outputId": "14eda8d8-7e5c-49f4-b488-e98a3045dd69"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/pointnet2/tf_ops/sampling')\n",
        "!bash tf_sampling_compile.sh"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[Ktf_sampling.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Ktf_sampling.cpp:20:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ktensorflow::Status tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, tensorflow::int64, tensorflow::shape_inference::ShapeHandle*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "     c->WithRank(c->input(0), 2, &dims1)\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ktf_sampling.cpp:8:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/shape_inference.h:394:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   Status \u001b[01;36m\u001b[KWithRank\u001b[m\u001b[K(ShapeHandle shape, int64 rank,\n",
            "          \u001b[01;36m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ktf_sampling.cpp:22:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ktensorflow::Status tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, tensorflow::int64, tensorflow::shape_inference::ShapeHandle*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "     c->WithRank(c->input(1), 2, &dims2)\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ktf_sampling.cpp:8:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/shape_inference.h:394:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   Status \u001b[01;36m\u001b[KWithRank\u001b[m\u001b[K(ShapeHandle shape, int64 rank,\n",
            "          \u001b[01;36m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ktf_sampling.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Ktf_sampling.cpp:34:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ktensorflow::Status tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, tensorflow::int64, tensorflow::shape_inference::ShapeHandle*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "     c->WithRank(c->input(0), 3, &dims1)\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ktf_sampling.cpp:8:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/shape_inference.h:394:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   Status \u001b[01;36m\u001b[KWithRank\u001b[m\u001b[K(ShapeHandle shape, int64 rank,\n",
            "          \u001b[01;36m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ktf_sampling.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Ktf_sampling.cpp:47:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ktensorflow::Status tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, tensorflow::int64, tensorflow::shape_inference::ShapeHandle*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "     c->WithRank(c->input(0), 3, &dims1)\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ktf_sampling.cpp:8:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/shape_inference.h:394:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   Status \u001b[01;36m\u001b[KWithRank\u001b[m\u001b[K(ShapeHandle shape, int64 rank,\n",
            "          \u001b[01;36m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ktf_sampling.cpp:49:40:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ktensorflow::Status tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, tensorflow::int64, tensorflow::shape_inference::ShapeHandle*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "     c->WithRank(c->input(1), 2, &dims2)\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "                                        \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ktf_sampling.cpp:8:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/shape_inference.h:394:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   Status \u001b[01;36m\u001b[KWithRank\u001b[m\u001b[K(ShapeHandle shape, int64 rank,\n",
            "          \u001b[01;36m\u001b[K^~~~~~~~\u001b[m\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa69K2G4PJf3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "c751a640-2714-4d48-b3b7-6ba746122182"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master//pointnet2/tf_ops/grouping')\n",
        "!bash tf_grouping_compile.sh"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[Ktf_grouping.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Ktf_grouping.cpp:22:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ktensorflow::Status tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, tensorflow::int64, tensorflow::shape_inference::ShapeHandle*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "         c->WithRank(c->input(1), 3, &dims2)\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ktf_grouping.cpp:8:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/shape_inference.h:394:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   Status \u001b[01;36m\u001b[KWithRank\u001b[m\u001b[K(ShapeHandle shape, int64 rank,\n",
            "          \u001b[01;36m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ktf_grouping.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Ktf_grouping.cpp:47:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ktensorflow::Status tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, tensorflow::int64, tensorflow::shape_inference::ShapeHandle*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "         c->WithRank(c->input(0), 3, &dims1)\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ktf_grouping.cpp:8:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/shape_inference.h:394:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   Status \u001b[01;36m\u001b[KWithRank\u001b[m\u001b[K(ShapeHandle shape, int64 rank,\n",
            "          \u001b[01;36m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ktf_grouping.cpp:49:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ktensorflow::Status tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, tensorflow::int64, tensorflow::shape_inference::ShapeHandle*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "         c->WithRank(c->input(1), 3, &dims2)\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ktf_grouping.cpp:8:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/shape_inference.h:394:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   Status \u001b[01;36m\u001b[KWithRank\u001b[m\u001b[K(ShapeHandle shape, int64 rank,\n",
            "          \u001b[01;36m\u001b[K^~~~~~~~\u001b[m\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oM2n6XaPhHz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "37667249-a52d-47aa-ea47-bee7e584c496"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master//pointnet2/tf_ops/3d_interpolation')\n",
        "!bash tf_interpolate_compile.sh"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[Ktf_interpolate.cpp:\u001b[m\u001b[K In lambda function:\n",
            "\u001b[01m\u001b[Ktf_interpolate.cpp:29:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ktensorflow::Status tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, tensorflow::int64, tensorflow::shape_inference::ShapeHandle*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "         c->WithRank(c->input(0), 3, &dims1)\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ktf_interpolate.cpp:8:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/shape_inference.h:394:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   Status \u001b[01;36m\u001b[KWithRank\u001b[m\u001b[K(ShapeHandle shape, int64 rank,\n",
            "          \u001b[01;36m\u001b[K^~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Ktf_interpolate.cpp:31:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kignoring return value of ‘\u001b[01m\u001b[Ktensorflow::Status tensorflow::shape_inference::InferenceContext::WithRank(tensorflow::shape_inference::ShapeHandle, tensorflow::int64, tensorflow::shape_inference::ShapeHandle*)\u001b[m\u001b[K’, declared with attribute warn_unused_result [\u001b[01;35m\u001b[K-Wunused-result\u001b[m\u001b[K]\n",
            "         c->WithRank(c->input(1), 3, &dims2)\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n",
            "                                            \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "In file included from \u001b[01m\u001b[Ktf_interpolate.cpp:8:0\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/shape_inference.h:394:10:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
            "   Status \u001b[01;36m\u001b[KWithRank\u001b[m\u001b[K(ShapeHandle shape, int64 rank,\n",
            "          \u001b[01;36m\u001b[K^~~~~~~~\u001b[m\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StR60KcFProc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZsc1KaIk9k1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "241a8bc6-671c-405a-db88-e41c042a9d10"
      },
      "source": [
        "# os.path.dirname('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/pointnet2/')\n",
        "sys.path"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/env/python',\n",
              " '/usr/lib/python36.zip',\n",
              " '/usr/lib/python3.6',\n",
              " '/usr/lib/python3.6/lib-dynload',\n",
              " '/usr/local/lib/python3.6/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.6/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOtbERWZQwUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_DIR = os.path.dirname('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/pointnet2/')\n",
        "ROOT_DIR = os.path.dirname('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/pointnet2/utils/')\n",
        "sys.path.append(BASE_DIR)\n",
        "sys.path.append(ROOT_DIR)\n",
        "# sys.path.append(os.path.join(ROOT_DIR, 'utils'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRq2mBn1jCZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tf_util\n",
        "import numpy as np\n",
        "import importlib\n",
        "\n",
        "import csv\n",
        "from pointnet_util import pointnet_sa_module, pointnet_sa_module_msg, pointnet_fp_module\n",
        "# from model_util import VAl_CONSTANT, VAL_NUM_HEADING_BIN, VAL_NUM_SIZE_CLUSTER\n",
        "# from model_util import NUM_HEADING_BIN, NUM_SIZE_CLUSTER, NUM_OBJECT_POINT\n",
        "# from model_util import point_cloud_masking, get_center_regression_net\n",
        "# from model_util import parse_output_to_tensors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0Cy2-XVjDK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_instance_classification_v1_net(point_cloud, is_training, bn_decay):\n",
        "#     l0_xyz = tf.slice(point_cloud, [0,0,0], [-1,-1,3])\n",
        "#     l0_points = tf.slice(point_cloud, [0,0,3], [-1,-1,1])\n",
        "\n",
        "#     # Set abstraction layers\n",
        "#     l1_xyz, l1_points = pointnet_sa_module_msg(l0_xyz, l0_points,\n",
        "#         128, [0.2,0.4,0.8], [32,64,128],\n",
        "#         [[32,32,64], [64,64,128], [64,96,128]],\n",
        "#         is_training, bn_decay, scope='layer1')\n",
        "#     l2_xyz, l2_points = pointnet_sa_module_msg(l1_xyz, l1_points,\n",
        "#         32, [0.4,0.8,1.6], [64,64,128],\n",
        "#         [[64,64,128], [128,128,256], [128,128,256]],\n",
        "#         is_training, bn_decay, scope='layer2')\n",
        "#     l3_xyz, l3_points, _ = pointnet_sa_module(l2_xyz, l2_points,\n",
        "#         npoint=None, radius=None, nsample=None, mlp=[128,256,1024],\n",
        "#         mlp2=None, group_all=True, is_training=is_training,\n",
        "#         bn_decay=bn_decay, scope='layer3')\n",
        "#     net = tf_util.fully_connected(tf.reshape(l3_points, [batch_size, 1024]), 512, scope='fc1-classification', bn=True, is_training=is_training, bn_decay=bn_decay)\n",
        "#     net = tf_util.fully_connected(net, 256, scope='fc2-classification', bn=True,\n",
        "#         is_training=is_training, bn_decay=bn_decay)\n",
        "#     predicted_classes_scores = tf_util.fully_connected(net, NUM_CLASSES, activation_fn=tf.nn.softmax,\n",
        "#         scope='fc3-classification')\n",
        "\n",
        "#     l3_points = tf.concat([l3_points, tf.expand_dims(predicted_classes_scores, 1)], axis=2)\n",
        "#     return l3_points, predicted_classes_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm1vPBRrGegV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_HEADING_BIN = 12\n",
        "NUM_SIZE_CLUSTER = 8 # one cluster for each type\n",
        "VAl_CONSTANT = 2 # 3 if 'x,y,z', 2 if 'x,y'\n",
        "VAL_NUM_HEADING_BIN = 2\n",
        "VAL_NUM_SIZE_CLUSTER = 3 # 4 if 'x,y,z', 3 if 'x,y'\n",
        "\n",
        "# Set training configurations\n",
        "BATCH_SIZE = 1\n",
        "NUM_POINT = 1024\n",
        "NUM_CLASSES = 2\n",
        "NUM_CHANNEL = 4\n",
        "NUM_OBJECT_POINT = 512\n",
        "xyz_only = 1\n",
        "\n",
        "batch_size = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsKwdP5zgIIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_gather_object_pc(point_cloud, mask, npoints=512):\n",
        "\t''' Gather object point clouds according to predicted masks.\n",
        "\tInput:\n",
        "\t\tpoint_cloud: TF tensor in shape (B,N,C)\n",
        "\t\tmask: TF tensor in shape (B,N) of 0 (not pick) or 1 (pick)\n",
        "\t\tnpoints: int scalar, maximum number of points to keep (default: 512)\n",
        "\tOutput:\n",
        "\t\tobject_pc: TF tensor in shape (B,npoint,C)\n",
        "\t\tindices: TF int tensor in shape (B,npoint,2)\n",
        "\t'''\n",
        "\tdef mask_to_indices(mask):\n",
        "\t\tindices = np.zeros((mask.shape[0], npoints, 2), dtype=np.int32)\n",
        "\t\tfor i in range(mask.shape[0]):\n",
        "\t\t\tpos_indices = np.where(mask[i,:]>0.5)[0] #BxN\n",
        "\t\t\t# skip cases when pos_indices is empty\n",
        "\t\t\tif len(pos_indices) > 0: \n",
        "\t\t\t\tif len(pos_indices) > npoints:\n",
        "\t\t\t\t\tchoice = np.random.choice(len(pos_indices),\n",
        "\t\t\t\t\t\tnpoints, replace=False)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tchoice = np.random.choice(len(pos_indices),\n",
        "\t\t\t\t\t\tnpoints-len(pos_indices), replace=True)\n",
        "\t\t\t\t\tchoice = np.concatenate((np.arange(len(pos_indices)), choice))\n",
        "\t\t\t\tnp.random.shuffle(choice)\n",
        "\t\t\t\tindices[i,:,1] = pos_indices[choice]\n",
        "\t\t\tindices[i,:,0] = i\n",
        "\t\treturn indices\n",
        "\n",
        "\tindices = tf.py_func(mask_to_indices, [mask], tf.int32)  \n",
        "\tobject_pc = tf.gather_nd(point_cloud, indices)\n",
        "\treturn object_pc, indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cckA0hxcHIDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "e6cdebcc-81f0-4a05-82ef-904de7da34a1"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "with tf.Graph().as_default():\n",
        "#   with tf.device('/gpu:0'):\n",
        "  is_training = tf.constant(False, dtype=tf.bool)\n",
        "  bn_decay=None\n",
        "  point_cloud = tf.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_POINT, 6))\n",
        "  batch_size = point_cloud.get_shape()[0].value\n",
        "  num_point = point_cloud.get_shape()[1].value\n",
        "  # Classification\n",
        "  l0_xyz = tf.slice(point_cloud, [0,0,0], [-1,-1,3])\n",
        "  l0_points = tf.slice(point_cloud, [0,0,3], [-1,-1,1])\n",
        "\n",
        "  # Set abstraction layers\n",
        "  l1_xyz, l1_points = pointnet_sa_module_msg(l0_xyz, l0_points,\n",
        "      128, [0.2,0.4,0.8], [32,64,128],\n",
        "      [[32,32,64], [64,64,128], [64,96,128]],\n",
        "      is_training, bn_decay, scope='layer1')\n",
        "  l2_xyz, l2_points = pointnet_sa_module_msg(l1_xyz, l1_points,\n",
        "      32, [0.4,0.8,1.6], [64,64,128],\n",
        "      [[64,64,128], [128,128,256], [128,128,256]],\n",
        "      is_training, bn_decay, scope='layer2')\n",
        "  l3_xyz, l3_points, _ = pointnet_sa_module(l2_xyz, l2_points,\n",
        "      npoint=None, radius=None, nsample=None, mlp=[128,256,1024],\n",
        "      mlp2=None, group_all=True, is_training=is_training,\n",
        "      bn_decay=bn_decay, scope='layer3')\n",
        "  net = tf_util.fully_connected(tf.reshape(l3_points, [batch_size, 1024]), 512, scope='fc1-classification', bn=True, is_training=is_training, bn_decay=bn_decay)\n",
        "  net = tf_util.fully_connected(net, 256, scope='fc2-classification', bn=True,\n",
        "      is_training=is_training, bn_decay=bn_decay)\n",
        "  predicted_classes_scores = tf_util.fully_connected(net, NUM_CLASSES, activation_fn=tf.nn.softmax,\n",
        "      scope='fc3-classification')\n",
        "\n",
        "  l3_points = tf.concat([l3_points, tf.expand_dims(predicted_classes_scores, 1)], axis=2)\n",
        "\n",
        "  # Segmentation\n",
        "  l2_points = pointnet_fp_module(l2_xyz, l3_xyz, l2_points, l3_points,\n",
        "      [128,128], is_training, bn_decay, scope='fa_layer1')\n",
        "  l1_points = pointnet_fp_module(l1_xyz, l2_xyz, l1_points, l2_points,\n",
        "      [128,128], is_training, bn_decay, scope='fa_layer2')\n",
        "  l0_points = pointnet_fp_module(l0_xyz, l1_xyz,\n",
        "      tf.concat([l0_xyz,l0_points],axis=-1), l1_points,\n",
        "      [128,128], is_training, bn_decay, scope='fa_layer3')\n",
        "\n",
        "  # FC layers\n",
        "  net = tf_util.conv1d(l0_points, 128, 1, padding='VALID', bn=True,\n",
        "      is_training=is_training, scope='conv1d-fc1', bn_decay=bn_decay)\n",
        "#     end_points['feats'] = net \n",
        "  net = tf_util.dropout(net, keep_prob=0.7,\n",
        "      is_training=is_training, scope='dp1')\n",
        "  logits = tf_util.conv1d(net, 2, 1,\n",
        "      padding='VALID', activation_fn=None, scope='conv1d-fc2')\n",
        "\n",
        "  # Masking\n",
        "  mask = tf.slice(logits,[0,0,0],[-1,-1,1]) < \\\n",
        "    tf.slice(logits,[0,0,1],[-1,-1,1]) #True when object is present else false\n",
        "  mask = tf.cast(mask, tf.float32) # BxNx1\n",
        "  mask_count = tf.tile(tf.reduce_sum(mask,axis=1,keep_dims=True),\n",
        "    [1,1,3]) # Bx1x3\n",
        "  point_cloud_xyz = tf.slice(point_cloud, [0,0,0], [-1,-1,3]) # BxNx3\n",
        "  mask_xyz_mean = tf.reduce_sum(tf.tile(mask, [1,1,3])*point_cloud_xyz,\n",
        "    axis=1, keep_dims=True) # Bx1x3 ##Computes mean of all the mask points 'x,y,z' which have a object 'mask' present in it\n",
        "  mask = tf.squeeze(mask, axis=[2]) # BxN\n",
        "  mask_xyz_mean = mask_xyz_mean/tf.maximum(mask_count,1) # Bx1x3\n",
        "\n",
        "  # Translate to masked points' centroid\n",
        "  point_cloud_xyz_stage1 = point_cloud_xyz - \\\n",
        "    tf.tile(mask_xyz_mean, [1,num_point,1])\n",
        "  if xyz_only: \n",
        "    point_cloud_stage1 = point_cloud_xyz_stage1\n",
        "  else:\n",
        "    point_cloud_features = tf.slice(point_cloud, [0,0,3], [-1,-1,-1])\n",
        "    point_cloud_stage1 = tf.concat(\\\n",
        "      [point_cloud_xyz_stage1, point_cloud_features], axis=-1)\n",
        "  num_channels = point_cloud_stage1.get_shape()[2].value\n",
        "  object_point_cloud, _ = tf_gather_object_pc(point_cloud_stage1,\n",
        "    mask, NUM_OBJECT_POINT)\n",
        "  object_point_cloud.set_shape([batch_size, NUM_OBJECT_POINT, num_channels])\n",
        "  #T-Net\n",
        "  num_point = object_point_cloud.get_shape()[1].value\n",
        "  net = tf.expand_dims(object_point_cloud, 2)\n",
        "  net = tf_util.conv2d(net, 128, [1,1],\n",
        "             padding='VALID', stride=[1,1],\n",
        "             bn=True, is_training=is_training,\n",
        "             scope='conv-reg1-stage1', bn_decay=bn_decay)\n",
        "  net = tf_util.conv2d(net,  128, [1,1],\n",
        "             padding='VALID', stride=[1,1],\n",
        "             bn=True, is_training=is_training,\n",
        "             scope='conv-reg2-stage1', bn_decay=bn_decay)\n",
        "  net = tf_util.conv2d(net, 256, [1,1],\n",
        "             padding='VALID', stride=[1,1],\n",
        "             bn=True, is_training=is_training,\n",
        "             scope='conv-reg3-stage1', bn_decay=bn_decay)\n",
        "  net = tf_util.max_pool2d(net, [num_point,1],\n",
        "    padding='VALID', scope='maxpool-stage1')\n",
        "  net = tf.squeeze(net, axis=[1,2])\n",
        "  net = tf.concat([net, predicted_classes_scores], axis=1)\n",
        "  net = tf_util.fully_connected(net, 256, scope='fc1-stage1', bn=True,\n",
        "    is_training=is_training, bn_decay=bn_decay)\n",
        "  net = tf_util.fully_connected(net, 128, scope='fc2-stage1', bn=True,\n",
        "    is_training=is_training, bn_decay=bn_decay)\n",
        "  predicted_center = tf_util.fully_connected(net, 3, activation_fn=None,\n",
        "    scope='fc3-stage1')\n",
        "  # Translation\n",
        "  stage1_center = predicted_center + mask_xyz_mean # Bx3\n",
        "\n",
        "  # Get object point cloud in object coordinate\n",
        "  object_point_cloud_xyz_new = \\\n",
        "    object_point_cloud - tf.expand_dims(predicted_center, 1)  \n",
        "  \n",
        "  # Amodal\n",
        "  batch_size = object_point_cloud_xyz_new.get_shape()[0].value\n",
        "\n",
        "  l0_xyz = object_point_cloud_xyz_new\n",
        "  l0_points = None\n",
        "  # Set abstraction layers\n",
        "  l1_xyz, l1_points, l1_indices = pointnet_sa_module(l0_xyz, l0_points,\n",
        "      npoint=128, radius=0.2, nsample=64, mlp=[64,64,128],\n",
        "      mlp2=None, group_all=False,\n",
        "      is_training=is_training, bn_decay=bn_decay, scope='ssg-layer1')\n",
        "  l2_xyz, l2_points, l2_indices = pointnet_sa_module(l1_xyz, l1_points,\n",
        "      npoint=32, radius=0.4, nsample=64, mlp=[128,128,256],\n",
        "      mlp2=None, group_all=False,\n",
        "      is_training=is_training, bn_decay=bn_decay, scope='ssg-layer2')\n",
        "  l3_xyz, l3_points, l3_indices = pointnet_sa_module(l2_xyz, l2_points,\n",
        "      npoint=None, radius=None, nsample=None, mlp=[256,256,512],\n",
        "      mlp2=None, group_all=True,\n",
        "      is_training=is_training, bn_decay=bn_decay, scope='ssg-layer3')\n",
        "\n",
        "  # Fully connected layers\n",
        "  net = tf.reshape(l3_points, [batch_size, -1])\n",
        "  net = tf.concat([net, predicted_classes_scores], axis=1)\n",
        "  net = tf_util.fully_connected(net, 512, bn=True,\n",
        "      is_training=is_training, scope='fc1', bn_decay=bn_decay)\n",
        "  net = tf_util.fully_connected(net, 256, bn=True,\n",
        "      is_training=is_training, scope='fc2', bn_decay=bn_decay)\n",
        "\n",
        "  # The first 3 numbers: box center coordinates (cx,cy,cz),\n",
        "  # the next NUM_HEADING_BIN*2:  heading bin class scores and bin residuals\n",
        "  # next NUM_SIZE_CLUSTER*4: box cluster scores and residuals\n",
        "  output = tf_util.fully_connected(net,\n",
        "      VAl_CONSTANT+VAL_NUM_HEADING_BIN*NUM_HEADING_BIN+VAL_NUM_SIZE_CLUSTER*NUM_SIZE_CLUSTER, activation_fn=None, scope='fc3')  \n",
        "  #################################\n",
        "\n",
        "  sess = tf.Session()\n",
        "  # with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  pc = np.loadtxt(open(\"/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/data_sample.csv\", \"rb\"), delimiter=\",\")\n",
        "#   print (pc)\n",
        "  ops = {'pointclouds_pl': point_cloud,\n",
        "           'is_training_pl': is_training}\n",
        "#     feed_dict = {ops['pointclouds_pl']: [pc], ops['is_training_pl']: False}\n",
        "  feed_dict = {ops['pointclouds_pl']: [pc]}\n",
        "  batch_size_scores = sess.run(output, feed_dict=feed_dict)\n",
        "  print(batch_size_scores.shape)\n",
        "  print(batch_size_scores)\n",
        "  # \t\tprint(VAl_CONSTANT+VAL_NUM_HEADING_BIN*NUM_HEADING_BIN+VAL_NUM_SIZE_CLUSTER*NUM_SIZE_CLUSTER)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 50)\n",
            "[[ 4.170571   -0.9892492  -0.49729058 -0.2758702   2.3102226  -0.4403419\n",
            "  -0.6460385  -0.7444961  -0.3599546   0.74409217 -0.9398809  -2.1423707\n",
            "  -3.6984768   2.2100174   2.0242653  -2.612691   -4.0998425  -2.413827\n",
            "  -0.8078402   0.6623677   0.09820342  0.9564769  -1.89658    -0.74844384\n",
            "   2.0782728  -0.11387211 -3.9579349  -3.2002017  -2.4371717  -0.2291559\n",
            "   1.1945076  -0.5025059  -2.1890934   2.1225324   2.5324304  -1.6708215\n",
            "   2.39565    -3.1280456  -0.18804705  1.3580453   4.673561    3.8168316\n",
            "  -0.22519392  1.3659784  -0.25010693  1.7068319  -3.5207624   0.3741892\n",
            "  -0.30280694  1.5922778 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KRmQJz-JYPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import random\n",
        "# import csv\n",
        "\n",
        "# with open('/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/data_sample.csv', 'w', newline='') as writeFile:\n",
        "# \twriter = csv.writer(writeFile)\n",
        "# \tfor x in range(1024):\n",
        "# \t\tline = \t[0,0,0,0.0,0.0,0.0]\n",
        "# \t\tline[0] = (random.randint(0,100))\n",
        "# \t\tline[1] = (random.randint(0,100))\n",
        "# \t\tline[2] = (random.randint(0,100))\n",
        "# \t\tline[3] = (random.random())\n",
        "# \t\tline[4] = (random.random())\n",
        "# \t\tline[5] = (random.random())\n",
        "# # \t\tprint (line)\n",
        "# \t\twriter.writerow(line)\n",
        "# pc = np.loadtxt(open(\"/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/data_sample.csv\", \"rb\"), delimiter=\",\")\n",
        "# print (pc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAd3Na_dJsCD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e90e7869-7cef-4775-e9fd-e0ffa6f17ebd"
      },
      "source": [
        "# Creates a graph.\n",
        "with tf.device('/gpu:0'):\n",
        "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
        "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
        "c = tf.matmul(a, b)\n",
        "# Creates a session with log_device_placement set to True.\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "# Runs the op.\n",
        "print(sess.run(c))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[22. 28.]\n",
            " [49. 64.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11czHB_-Qugw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "778203f0-2221-40d7-d32e-57db444dc5d9"
      },
      "source": [
        "!cat \"/content/gdrive/My Drive/pointnet/PointnetEnhanced-master/pointnet2/utils/pointnet_util.py\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"\"\" PointNet++ Layers\n",
            "\n",
            "Author: Charles R. Qi\n",
            "Date: November 2017\n",
            "\"\"\"\n",
            "\n",
            "import os\n",
            "import sys\n",
            "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
            "ROOT_DIR = os.path.dirname(BASE_DIR)\n",
            "sys.path.append(os.path.join(ROOT_DIR, 'utils'))\n",
            "sys.path.append(os.path.join(ROOT_DIR, 'tf_ops/sampling'))\n",
            "sys.path.append(os.path.join(ROOT_DIR, 'tf_ops/grouping'))\n",
            "sys.path.append(os.path.join(ROOT_DIR, 'tf_ops/3d_interpolation'))\n",
            "from tf_sampling import farthest_point_sample, gather_point\n",
            "from tf_grouping import query_ball_point, group_point, knn_point\n",
            "from tf_interpolate import three_nn, three_interpolate\n",
            "import tensorflow as tf\n",
            "import numpy as np\n",
            "import tf_util\n",
            "\n",
            "def sample_and_group(npoint, radius, nsample, xyz, points, knn=False, use_xyz=True):\n",
            "    '''\n",
            "    Input:\n",
            "        npoint: int32\n",
            "        radius: float32\n",
            "        nsample: int32\n",
            "        xyz: (batch_size, ndataset, 3) TF tensor\n",
            "        points: (batch_size, ndataset, channel) TF tensor, if None will just use xyz as points\n",
            "        knn: bool, if True use kNN instead of radius search\n",
            "        use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n",
            "    Output:\n",
            "        new_xyz: (batch_size, npoint, 3) TF tensor\n",
            "        new_points: (batch_size, npoint, nsample, 3+channel) TF tensor\n",
            "        idx: (batch_size, npoint, nsample) TF tensor, indices of local points as in ndataset points\n",
            "        grouped_xyz: (batch_size, npoint, nsample, 3) TF tensor, normalized point XYZs\n",
            "            (subtracted by seed point XYZ) in local regions\n",
            "    '''\n",
            "\n",
            "    new_xyz = gather_point(xyz, farthest_point_sample(npoint, xyz)) # (batch_size, npoint, 3)\n",
            "    if knn:\n",
            "        _,idx = knn_point(nsample, xyz, new_xyz)\n",
            "    else:\n",
            "        idx, pts_cnt = query_ball_point(radius, nsample, xyz, new_xyz)\n",
            "    grouped_xyz = group_point(xyz, idx) # (batch_size, npoint, nsample, 3)\n",
            "    grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2), [1,1,nsample,1]) # translation normalization\n",
            "    if points is not None:\n",
            "        grouped_points = group_point(points, idx) # (batch_size, npoint, nsample, channel)\n",
            "        if use_xyz:\n",
            "            new_points = tf.concat([grouped_xyz, grouped_points], axis=-1) # (batch_size, npoint, nample, 3+channel)\n",
            "        else:\n",
            "            new_points = grouped_points\n",
            "    else:\n",
            "        new_points = grouped_xyz\n",
            "\n",
            "    return new_xyz, new_points, idx, grouped_xyz\n",
            "\n",
            "\n",
            "def sample_and_group_all(xyz, points, use_xyz=True):\n",
            "    '''\n",
            "    Inputs:\n",
            "        xyz: (batch_size, ndataset, 3) TF tensor\n",
            "        points: (batch_size, ndataset, channel) TF tensor, if None will just use xyz as points\n",
            "        use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n",
            "    Outputs:\n",
            "        new_xyz: (batch_size, 1, 3) as (0,0,0)\n",
            "        new_points: (batch_size, 1, ndataset, 3+channel) TF tensor\n",
            "    Note:\n",
            "        Equivalent to sample_and_group with npoint=1, radius=inf, use (0,0,0) as the centroid\n",
            "    '''\n",
            "    batch_size = xyz.get_shape()[0].value\n",
            "    nsample = xyz.get_shape()[1].value\n",
            "    new_xyz = tf.constant(np.tile(np.array([0,0,0]).reshape((1,1,3)), (batch_size,1,1)),dtype=tf.float32) # (batch_size, 1, 3)\n",
            "    idx = tf.constant(np.tile(np.array(range(nsample)).reshape((1,1,nsample)), (batch_size,1,1)))\n",
            "    grouped_xyz = tf.reshape(xyz, (batch_size, 1, nsample, 3)) # (batch_size, npoint=1, nsample, 3)\n",
            "    if points is not None:\n",
            "        if use_xyz:\n",
            "            new_points = tf.concat([xyz, points], axis=2) # (batch_size, 16, 259)\n",
            "        else:\n",
            "            new_points = points\n",
            "        new_points = tf.expand_dims(new_points, 1) # (batch_size, 1, 16, 259)\n",
            "    else:\n",
            "        new_points = grouped_xyz\n",
            "    return new_xyz, new_points, idx, grouped_xyz\n",
            "\n",
            "\n",
            "def pointnet_sa_module(xyz, points, npoint, radius, nsample, mlp, mlp2, group_all, is_training, bn_decay, scope, bn=True, pooling='max', knn=False, use_xyz=True, use_nchw=False):\n",
            "    ''' PointNet Set Abstraction (SA) Module\n",
            "        Input:\n",
            "            xyz: (batch_size, ndataset, 3) TF tensor\n",
            "            points: (batch_size, ndataset, channel) TF tensor\n",
            "            npoint: int32 -- #points sampled in farthest point sampling\n",
            "            radius: float32 -- search radius in local region\n",
            "            nsample: int32 -- how many points in each local region\n",
            "            mlp: list of int32 -- output size for MLP on each point\n",
            "            mlp2: list of int32 -- output size for MLP on each region\n",
            "            group_all: bool -- group all points into one PC if set true, OVERRIDE\n",
            "                npoint, radius and nsample settings\n",
            "            use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n",
            "            use_nchw: bool, if True, use NCHW data format for conv2d, which is usually faster than NHWC format\n",
            "        Return:\n",
            "            new_xyz: (batch_size, npoint, 3) TF tensor\n",
            "            new_points: (batch_size, npoint, mlp[-1] or mlp2[-1]) TF tensor\n",
            "            idx: (batch_size, npoint, nsample) int32 -- indices for local regions\n",
            "    '''\n",
            "    data_format = 'NCHW' if use_nchw else 'NHWC'\n",
            "    with tf.variable_scope(scope) as sc:\n",
            "        # Sample and Grouping\n",
            "        if group_all:\n",
            "            nsample = xyz.get_shape()[1].value\n",
            "            new_xyz, new_points, idx, grouped_xyz = sample_and_group_all(xyz, points, use_xyz)\n",
            "        else:\n",
            "            new_xyz, new_points, idx, grouped_xyz = sample_and_group(npoint, radius, nsample, xyz, points, knn, use_xyz)\n",
            "\n",
            "        # Point Feature Embedding\n",
            "        if use_nchw: new_points = tf.transpose(new_points, [0,3,1,2])\n",
            "        for i, num_out_channel in enumerate(mlp):\n",
            "            new_points = tf_util.conv2d(new_points, num_out_channel, [1,1],\n",
            "                                        padding='VALID', stride=[1,1],\n",
            "                                        bn=bn, is_training=is_training,\n",
            "                                        scope='conv%d'%(i), bn_decay=bn_decay,\n",
            "                                        data_format=data_format) \n",
            "        if use_nchw: new_points = tf.transpose(new_points, [0,2,3,1])\n",
            "\n",
            "        # Pooling in Local Regions\n",
            "        if pooling=='max':\n",
            "            new_points = tf.reduce_max(new_points, axis=[2], keep_dims=True, name='maxpool')\n",
            "        elif pooling=='avg':\n",
            "            new_points = tf.reduce_mean(new_points, axis=[2], keep_dims=True, name='avgpool')\n",
            "        elif pooling=='weighted_avg':\n",
            "            with tf.variable_scope('weighted_avg'):\n",
            "                dists = tf.norm(grouped_xyz,axis=-1,ord=2,keep_dims=True)\n",
            "                exp_dists = tf.exp(-dists * 5)\n",
            "                weights = exp_dists/tf.reduce_sum(exp_dists,axis=2,keep_dims=True) # (batch_size, npoint, nsample, 1)\n",
            "                new_points *= weights # (batch_size, npoint, nsample, mlp[-1])\n",
            "                new_points = tf.reduce_sum(new_points, axis=2, keep_dims=True)\n",
            "        elif pooling=='max_and_avg':\n",
            "            max_points = tf.reduce_max(new_points, axis=[2], keep_dims=True, name='maxpool')\n",
            "            avg_points = tf.reduce_mean(new_points, axis=[2], keep_dims=True, name='avgpool')\n",
            "            new_points = tf.concat([avg_points, max_points], axis=-1)\n",
            "\n",
            "        # [Optional] Further Processing \n",
            "        if mlp2 is not None:\n",
            "            if use_nchw: new_points = tf.transpose(new_points, [0,3,1,2])\n",
            "            for i, num_out_channel in enumerate(mlp2):\n",
            "                new_points = tf_util.conv2d(new_points, num_out_channel, [1,1],\n",
            "                                            padding='VALID', stride=[1,1],\n",
            "                                            bn=bn, is_training=is_training,\n",
            "                                            scope='conv_post_%d'%(i), bn_decay=bn_decay,\n",
            "                                            data_format=data_format) \n",
            "            if use_nchw: new_points = tf.transpose(new_points, [0,2,3,1])\n",
            "\n",
            "        new_points = tf.squeeze(new_points, [2]) # (batch_size, npoints, mlp2[-1])\n",
            "        return new_xyz, new_points, idx\n",
            "\n",
            "def pointnet_sa_module_msg(xyz, points, npoint, radius_list, nsample_list, mlp_list, is_training, bn_decay, scope, bn=True, use_xyz=True, use_nchw=False):\n",
            "    ''' PointNet Set Abstraction (SA) module with Multi-Scale Grouping (MSG)\n",
            "        Input:\n",
            "            xyz: (batch_size, ndataset, 3) TF tensor\n",
            "            points: (batch_size, ndataset, channel) TF tensor\n",
            "            npoint: int32 -- #points sampled in farthest point sampling\n",
            "            radius: list of float32 -- search radius in local region\n",
            "            nsample: list of int32 -- how many points in each local region\n",
            "            mlp: list of list of int32 -- output size for MLP on each point\n",
            "            use_xyz: bool, if True concat XYZ with local point features, otherwise just use point features\n",
            "            use_nchw: bool, if True, use NCHW data format for conv2d, which is usually faster than NHWC format\n",
            "        Return:\n",
            "            new_xyz: (batch_size, npoint, 3) TF tensor\n",
            "            new_points: (batch_size, npoint, \\sum_k{mlp[k][-1]}) TF tensor\n",
            "    '''\n",
            "    data_format = 'NCHW' if use_nchw else 'NHWC'\n",
            "    with tf.variable_scope(scope) as sc:\n",
            "        new_xyz = gather_point(xyz, farthest_point_sample(npoint, xyz))\n",
            "        new_points_list = []\n",
            "        for i in range(len(radius_list)):\n",
            "            radius = radius_list[i]\n",
            "            nsample = nsample_list[i]\n",
            "            idx, pts_cnt = query_ball_point(radius, nsample, xyz, new_xyz)\n",
            "            grouped_xyz = group_point(xyz, idx)\n",
            "            grouped_xyz -= tf.tile(tf.expand_dims(new_xyz, 2), [1,1,nsample,1])\n",
            "            if points is not None:\n",
            "                grouped_points = group_point(points, idx)\n",
            "                if use_xyz:\n",
            "                    grouped_points = tf.concat([grouped_points, grouped_xyz], axis=-1)\n",
            "            else:\n",
            "                grouped_points = grouped_xyz\n",
            "            if use_nchw: grouped_points = tf.transpose(grouped_points, [0,3,1,2])\n",
            "            for j,num_out_channel in enumerate(mlp_list[i]):\n",
            "                grouped_points = tf_util.conv2d(grouped_points, num_out_channel, [1,1],\n",
            "                                                padding='VALID', stride=[1,1], bn=bn, is_training=is_training,\n",
            "                                                scope='conv%d_%d'%(i,j), bn_decay=bn_decay)\n",
            "            if use_nchw: grouped_points = tf.transpose(grouped_points, [0,2,3,1])\n",
            "            new_points = tf.reduce_max(grouped_points, axis=[2])\n",
            "            new_points_list.append(new_points)\n",
            "        new_points_concat = tf.concat(new_points_list, axis=-1)\n",
            "        return new_xyz, new_points_concat\n",
            "\n",
            " \n",
            "def pointnet_fp_module(xyz1, xyz2, points1, points2, mlp, is_training, bn_decay, scope, bn=True):\n",
            "    ''' PointNet Feature Propogation (FP) Module\n",
            "        Input:                                                                                                      \n",
            "            xyz1: (batch_size, ndataset1, 3) TF tensor                                                              \n",
            "            xyz2: (batch_size, ndataset2, 3) TF tensor, sparser than xyz1                                           \n",
            "            points1: (batch_size, ndataset1, nchannel1) TF tensor                                                   \n",
            "            points2: (batch_size, ndataset2, nchannel2) TF tensor\n",
            "            mlp: list of int32 -- output size for MLP on each point                                                 \n",
            "        Return:\n",
            "            new_points: (batch_size, ndataset1, mlp[-1]) TF tensor\n",
            "    '''\n",
            "    with tf.variable_scope(scope) as sc:\n",
            "        dist, idx = three_nn(xyz1, xyz2)\n",
            "        dist = tf.maximum(dist, 1e-10)\n",
            "        norm = tf.reduce_sum((1.0/dist),axis=2,keep_dims=True)\n",
            "        norm = tf.tile(norm,[1,1,3])\n",
            "        weight = (1.0/dist) / norm\n",
            "        interpolated_points = three_interpolate(points2, idx, weight)\n",
            "\n",
            "        if points1 is not None:\n",
            "            new_points1 = tf.concat(axis=2, values=[interpolated_points, points1]) # B,ndataset1,nchannel1+nchannel2\n",
            "        else:\n",
            "            new_points1 = interpolated_points\n",
            "        new_points1 = tf.expand_dims(new_points1, 2)\n",
            "        for i, num_out_channel in enumerate(mlp):\n",
            "            new_points1 = tf_util.conv2d(new_points1, num_out_channel, [1,1],\n",
            "                                         padding='VALID', stride=[1,1],\n",
            "                                         bn=bn, is_training=is_training,\n",
            "                                         scope='conv_%d'%(i), bn_decay=bn_decay)\n",
            "        new_points1 = tf.squeeze(new_points1, [2]) # B,ndataset1,mlp[-1]\n",
            "        return new_points1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8tProSwWeJy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ":"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}